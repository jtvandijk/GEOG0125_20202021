[
["index.html", "GEOG0125: Advanced Topics in Social and Geographic Data Science Welcome Weekly topics Moodle Useful additional resources Office hours", " GEOG0125: Advanced Topics in Social and Geographic Data Science Justin van Dijk1 and Stephen Law2 2021-03-06 Welcome Welcome to Advanced Topics in Social and Geographic Data Science, one of the elective modules for the MSc in Geographic and Social Data Science here at UCL Geography.This module has been designed as an advanced topics module to learn data science concepts and methods and to apply them in the domains of social science and geography. The first half of the module introduces a variety of important concepts such as statistical inference, deep learning, and convolutional neural networks. We will end with a keynote lecture from Carto on Spatial Data Science. In the second half of the module, we will introduce you to the practical skills of being a data scientists including an introduction to databases, unix-tools and web visualisation (a data sciencist’s toolbox). We hope this module can provide you with the necessary knowledge in becoming a social and geographic data scientist in the future. Weekly topics Week Date Topic Moodle GitHub 1 11/01/2021 Introduction to SocGeoDataScience x 2 18/01/2021 Statistical Inference and Causality x 3 25/01/2021 GeoAI: Introduction to Deep Learning x 4 01/02/2021 Keynote: Spatial Data Science (Carto) x 5 08/02/2021 GeoAI: Convolutional Neural Networks x reading week 6 22/02/2021 GeoAI: Spatial Data Science Applications x 7 01/03/2021 GeoAI: Spatial-Temporal Mobility Analysis x 8 08/03/2021 Data Sciencist’s Toolbox: Unix tools x 9 15/03/2021 Data Sciencist’s Toolbox: Web Visualisation x 10 22/03/2021 A look into the future of SocGeoDataScience x Moodle All important information and communication in relation to this module will be provided on Moodle. Even though some of the short lecture videos and practical materials will be hosted on this webpage, do check on Moodle regularly to see if there are any updates or important messages. Useful additional resources Besides the mandatory and recommended reading for this course, there are some additional resources that are worth checking out: MIT’s introduction course on mastering the command line: The Missing Semester of Your CS Education A useful tool to unpack command line instructions: explainshell.com Online resource to develop and check your regular expressions: regexr.com Selecting colour palettes for your map making and data visualisation: colorbrewer 2.0 Office hours Office hours with Dr Justin van Dijk (Tuesdays between 15h00 and 16h00) can be booked here. Department of Geography, https://www.mappingdutchman.com/↩︎ Department of Geography, https://www.geog.ucl.ac.uk/people/academic-staff/stephen-law↩︎ "],
["introduction.html", "1 Introduction", " 1 Introduction The first week of Advanced Topics in Social and Geographic Data Science will introduce the module, why is it useful and what you will learn. You will be given a recap to the introduction to social and geographic data science specifically on the machine learning workflow. We will then give instructions to the reading group and the coursework assessment. This week’s practical component will focus on installing a new virtual environment for the module, accessing Jupyter Notebook locally and a recap on a typical machine learning workflow in Python. We will also get everyone familiar with Google Colab Web service. This week plays a formative role in providing you with the baseline knowledge for the module. This week’s material is available on Moodle. "],
["statistical-inference-and-causality.html", "2 Statistical Inference and Causality 2.1 Introduction 2.2 Statistical inference 2.3 Causality 2.4 Take home message 2.5 Attributions", " 2 Statistical Inference and Causality 2.1 Introduction Statistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. In other words: statistical inference is the procedure through which we try to make inferences about a population based on characteristics of this population that have been captured in a sample. This includes uncovering the association between variables as well as establishing causal relationships. However, correlation does not imply causation and identifying causal relationships from observational data is not trivial. “Correlation does not imply causation” (Any statistician). This week we will dive a little deeper into statistical inference and how to establish causal relationships by taking a small dive into the field of study of econometrics. This week’s lecture video is provided by an absolute expert in the field of econometrics: Dr Idahosa from the University of Johannesburg. This week is further structured by reading material, a tutorial in R with a ‘hands-on’ application of the techniques covered in the lecture video, and a seminar on Tuesday. Let’s get to it! Video: Introduction W01 [Lecture slides] [Watch on MS stream] 2.1.1 Reading list Please find the reading list for this week below. Core reading Bzdok, D., Altman, and M. Krzywinski. 2018. Statistics versus machine learning. Nature Methods 15: 233-234. [Link] Idahosa, L. O., Marwa, N., and J. Akotey. 2017. Energy (electricity) consumption in South African hotels: A panel data analysis. Energy and Buildings 156: 207-217. [Link] Stock, J. and M. Watson. 2019. Chapter 1: Economic Questions and Data. In: Stock, J. and M. Watson. Introduction to Econometrics, pp.43-54. Harlow: Pearson Education Ltd. [Link] Stock, J. and M. Watson. 2019. Chapter 10: Regression with Panel Data. In: Stock, J. and M. Watson. Introduction to Econometrics, pp.362-382. Harlow: Pearson Education Ltd. [Link] Supplementary reading Idahosa, L. O. and J. T. van Dijk. 2016. South Africa: Freedom for whom? Inequality, unemployment and the elderly. Development 58(1): 96-102. [Link] 2.1.2 Technical Help session Every Thursday between 13h00-14h00 you can join the Technical Help session on Microsoft Teams. The session will be hosted by Alfie. He will be there for the whole hour to answer any question you have live in the meeting or any questions you have formulated beforehand. If you cannot make the meeting, feel free to post the issue you are having in the Technical Help channel on the GEOG0125 Team so that Alfie can help to find a solution. 2.2 Statistical inference Where during the remainder of this module we will predominantly focus on different machine learning methods and techniques and the data scientist’s toolbox, this week we will focus on statistical inference. Although there is considerable overlap between machine learning and statistics, sometimes even involving the same methods, the major difference between machine learning and statistics is their purpose. Where machine learning models are designed to make accurate predictions, statistical models are designed for inference about the relationships between variables. “Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns” (Bzdok et al. 2018). One way to analyse the relationships between variables is by a linear regression. Linear regression models offer a very interpretable way to model the association between variables. A linear regression model is used to find the line that minimises the mean squared error across all data to establish the relationship between a response variable one or more explanatory (independent) variables. The case of one explanatory variable is called a simple linear regression; if more explanatory variables are used it is called multiple regression. The purpose of a regression model is to predict a target variable \\(Y\\) according to some other variable \\(X\\) or variables \\(X_{1}\\), \\(X_{2}\\), etc. 2.3 Causality 2.3.1 What is causality? Regression cares about correlation: what happens to \\(Y\\) when \\(X=x\\). However, correlation does not imply causation. A great example can be found in a research project reporting on the relationships between chocolate consumption and the probability of winning a Nobel price. The results suggest that countries in which the population consumes on average a large amount of chocolate per annum, spawn more Nobel laureates per capita than countries in which the population on average consume less chocolate. There is a clear correlation in the data, but there should not exist an actual causal relationship between these two variables (Well, some research suggests it still remains unclear whether the correlation is spurious or whether it is an indication for hidden variables …) The golden standard to establish a causal relationship is to set-up and execute a randomised control trial. Think of the many large-scale randomised control trials that are currently taking place to test the safety and effectiveness of various candidate coronavirus vaccines. However, it is not always possible to set up a randomised control trial. Sometimes this has to do with the nature of the relationship being investigated (e.g. establishing the effects of policy changes), but there could also be financial and ethical constraints. As an alternative, one could try to identify causal relationship from observational data. This is known as causal inference and most research in econometrics is concerned with retrieving valid estimates using different regression methods. Note The distinction between causal and non-causal relationships is crucial and heavily depends on your research questions. If you are trying to classify Google Street view images and predict whether the photo contains an office building or a residential building, you want to create a model that predicts this as good as possible. However, you do not care about what caused the building in the photo to be an office building or a residential building. That being said: almost any question is causal and where statistics is used in almost any field of inquiry, few pay proper attention to understanding causality. By now you may wonder, sure, but what then is causality? We can say that \\(X\\) causes \\(Y\\) if we were to change the value of \\(X\\) without changing anything else then as a result \\(Y\\) would also change. A simple example: if you switch on the light switch, your light bulb will go on. The action of flipping the light switch causes your light to go on. This being said: it does not mean that \\(X\\) is necessarily the only thing that causes \\(Y\\) (e.g. the light bulb is burnt out or the light was already on) and perhaps a better way of phrasing it is to say that there is a causal relationship between variables if \\(X\\) changes the probability of \\(Y\\) happening or changing. 2.3.2 The problem with causality The problem with establishing a causal relationship is that in many cases you cannot ‘switch on’ or ‘switch off’ a characteristic. Let’s go through this by thinking whether some \\(X\\) causes \\(Y\\). Our \\(X\\) is coded as 0 or 1, for instance, 0 if a person has not received a coronavirus vaccination and 1 if a person has received a coronavirus vaccination. \\(Y\\) is some numeric value. So how do we check if \\(X\\) causes \\(Y\\)? What we would need to do for everyone in our sample is to check what happens to \\(Y\\) when we make \\(X=0\\) and what happens to \\(Y\\) when we make \\(X=1\\). Obviously, this is a problem! You cannot both have \\(X=0\\) and \\(X=1\\): you either got inoculated against the coronavirus or you did not. This means that if \\(X=1\\) you can measure what the value of \\(Y\\) is, but you do not know what the value of \\(Y\\) would have been if \\(X=0\\). The solution you may come up with is to compare \\(Y\\) between individuals who have \\(X=0\\) and \\(X=1\\). However, there is another problem: there could be all kinds of reasons on why \\(Y\\) differs between individuals that are not necessarily related to \\(X\\). Note This section heavily borrows material and explanations from Nick Huntington-Klein’s excellent ECON 305: Economics, Causality, and Analytics module, do have a look if you want to learn more about this topic. This brings us to econometrics and causal inference: the main goal of causal inference is to make the best possible estimation of what \\(Y\\) would have been if \\(X\\) would have been different, the so-called counterfactual. As we cannot always use an experiment in which we can randomly assign \\(X\\) so that we know that on average people with \\(X=1\\) and the same as people with \\(X=2\\), we have to come up with a model to figure out what the counterfactual would do. In the following, we will explore two ways of doing this through so-called fixed effects models and random effects models in the situation in which we have data points for each observation across time (i.e. longitudinal data or panel data). Video: Panel Data Analysis [Lecture slides] [Watch on MS stream] 2.3.3 Fixed effects models Fixed effects are variables that are constant across individuals; these variables, like age, sex, or ethnicity, typically do not change over time or change over time at a constant rate. As such, they have fixed effects on a dependent variable \\(Y\\). As such, using a fixed effects model you can explore the relationship between variables within an entity (which could be persons, companies, countries, etc.). Each entity has its own individual characteristics that may or may not influence the dependent variable. When using a fixed effects model, we assume that something within the entity may impact or bias the dependent variables and we need to control for this. A fixed effects model does this by removing the characteristics that do not change over time so that we can assess the net effect of the independent variables on the dependent variable. Let’s try to to apply a fixed effects model in R. For this we will use a data set containing some panel data. The data set contains fictional data, for different countries and years, for an undefined variable \\(Y\\) that we want to explain with some other variables. File download File Type Link Example Panel Data csv Download # load libraries library(tidyverse) library(plm) library(car) library(gplots) library(tseries) library(lmtest) # read data country_data &lt;- read_csv(&#39;raw/w02/paneldata.csv&#39;) # inspect head(country_data) ## # A tibble: 6 x 6 ## country year y x1 x2 x3 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1990 1342787840 0.278 -1.11 0.283 ## 2 A 1991 -1899660544 0.321 -0.949 0.493 ## 3 A 1992 -11234363 0.363 -0.789 0.703 ## 4 A 1993 2645775360 0.246 -0.886 -0.0944 ## 5 A 1994 3008334848 0.425 -0.730 0.946 ## 6 A 1995 3229574144 0.477 -0.723 1.03 Upon inspecting the dataframe you can see that the data contains 8 different fictional countries. For each country we have several years of data: three independent variables names \\(X_{1}\\), \\(X_{2}\\), and \\(X_{3}\\) and a dependent variable \\(y\\). As this is a panel dataset we have to declare it as such using the plm.data() function from the plm package. The plm package is a library dedicated to panel data analysis. # create a panel data object country_panel &lt;- pdata.frame(country_data, index=c(&#39;country&#39;,&#39;year&#39;)) # inspect country_panel ## country year y x1 x2 x3 ## A-1990 A 1990 1342787840 0.2779037 -1.1079559 0.28255358 ## A-1991 A 1991 -1899660544 0.3206847 -0.9487200 0.49253848 ## A-1992 A 1992 -11234363 0.3634657 -0.7894840 0.70252335 ## A-1993 A 1993 2645775360 0.2461440 -0.8855330 -0.09439092 ## A-1994 A 1994 3008334848 0.4246230 -0.7297683 0.94613063 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 65 rows ] Although the data looks the same, you can see that the row index has been updated to reflect the country and year variables. Let’s inspect the data using a boxplot as well as a conditioning plot. A coplot is a method for visualising interactions in your data set: it shows you how some variables are conditional on some other set of variables. So, for our panel data set, we can look at the variation of \\(Y\\) over time by country. The bars at top indicate the countries position from left to right starting on the bottom row. # create a quick box plot scatterplot(y ~ year, data=country_panel) ## [1] &quot;11&quot; &quot;54&quot; &quot;45&quot; &quot;55&quot; &quot;46&quot; &quot;36&quot; &quot;59&quot; # create a quick conditioning plot coplot(y ~ year|country, data=country_panel, type=&#39;b&#39;) The graphs show that there \\(Y\\) is variable both over time and between countries. Let’s also have a look at the heterogeneity of our data by plotting the means, and the 95% confidence interval around the means, of \\(Y\\) across time and across countries. # means across years plotmeans(y ~ year, data=country_panel) # means across countries plotmeans(y ~ country, data=country_panel) There is clearly some heterogeneity across the countries and across years. However, the basic Ordinary Least Squares (OLS) regression model does not consider this heterogeneity: # run an OLS ols &lt;- lm(y ~ x1, data=country_panel) # summary summary(ols) ## ## Call: ## lm(formula = y ~ x1, data = country_panel) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.546e+09 -1.578e+09 1.554e+08 1.422e+09 7.183e+09 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.524e+09 6.211e+08 2.454 0.0167 * ## x1 4.950e+08 7.789e+08 0.636 0.5272 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.028e+09 on 68 degrees of freedom ## Multiple R-squared: 0.005905,\tAdjusted R-squared: -0.008714 ## F-statistic: 0.4039 on 1 and 68 DF, p-value: 0.5272 The results show that there is no significant statistical relationship between \\(X_{1}\\) and the dependent variable \\(Y\\) as the model tries to explain all variability in the data at once. You can clearly see this in the plot below: # plot the results scatterplot(country_panel$y ~ country_panel$x1, boxplots=FALSE, smooth=FALSE, pch=19, col=&#39;black&#39;) # add the OLS regression line abline(lm(y ~ x1, data=country_panel),lwd=3, col=&#39;red&#39;) One way of getting around this, and fixing the effects of the country variable, is by using a model incorporating dummy variables through a Least Squares Dummy Variable model (LSDV). # run a LSDV fixed_dum &lt;- lm(y ~ x1 + factor(country) - 1, data=country_panel) # summary summary(fixed_dum) ## ## Call: ## lm(formula = y ~ x1 + factor(country) - 1, data = country_panel) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.634e+09 -9.697e+08 5.405e+08 1.386e+09 5.612e+09 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x1 2.476e+09 1.107e+09 2.237 0.02889 * ## factor(country)A 8.805e+08 9.618e+08 0.916 0.36347 ## factor(country)B -1.058e+09 1.051e+09 -1.006 0.31811 ## factor(country)C -1.723e+09 1.632e+09 -1.056 0.29508 ## factor(country)D 3.163e+09 9.095e+08 3.478 0.00093 *** ## factor(country)E -6.026e+08 1.064e+09 -0.566 0.57329 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.796e+09 on 62 degrees of freedom ## Multiple R-squared: 0.4402,\tAdjusted R-squared: 0.368 ## F-statistic: 6.095 on 8 and 62 DF, p-value: 8.892e-06 The least square dummy variable model (LSDV) provides a good way to understand fixed effects. By adding the dummy for each country, we are now estimating the pure effect of \\(X_{1}\\) on \\(Y\\). Each dummy is absorbing the effects that are particular to each country. Where the independent variable \\(X_{1}\\) was not significant in the OLS model, after controlling for differences across countries, \\(X_{1}\\) became significant in the LSDV model. You can clearly see why this is happening in the plot below: # plot the results scatterplot(fixed_dum$fitted ~ country_panel$x1|country_panel$country, boxplots=FALSE, smooth=FALSE, col=&#39;black&#39;) ## Warning in scatterplot.default(X[, 2], X[, 1], groups = X[, 3], xlab = xlab, : number of groups exceeds number of available colors ## colors are recycled # add the LSDV regression lines abline(lm(y ~ x1, data=country_panel),lwd=3, col=&#39;red&#39;) We can also run a country-specific fixed effects model by using specific intercepts for each country. We can achieve this by using the plm package. # run a fixed effects model fixed_effects &lt;- plm(y ~ x1, data=country_panel, model=&#39;within&#39;) # summary summary(fixed_effects) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = y ~ x1, data = country_panel, model = &quot;within&quot;) ## ## Balanced Panel: n = 7, T = 10, N = 70 ## ## Residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -8.63e+09 -9.70e+08 5.40e+08 0.00e+00 1.39e+09 5.61e+09 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## x1 2475617742 1106675596 2.237 0.02889 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 5.2364e+20 ## Residual Sum of Squares: 4.8454e+20 ## R-Squared: 0.074684 ## Adj. R-Squared: -0.029788 ## F-statistic: 5.00411 on 1 and 62 DF, p-value: 0.028892 Here you can check the individual intercepts through fixef(fixed). The coefficient of \\(X_{1}\\) indicates how much \\(Y\\) changes on average over time per country: as you can see the results are identical to the results we got from running the LSDV model. Arguably, however, running your model with explicit dummy variables is more informative. 2.3.4 Random effects models Random effects are the opposite of fixed effects. Contrary to fixed effects, random effects are random and difficult to predict. As such, the effect they will have on a dependent variable \\(Y\\) is not constant. Think of the cost of renting a one bedroom appartement: rental prices vary greatly depending on location. If you have reason to believe that differences across entities have some influence on the dependent variable that is not time-dependent, then you would use a random effects model approach over a fixed effects model approach. # run a random effects model random_effects &lt;- plm(y ~ x1, data=country_panel, model=&#39;random&#39;) # summary summary(random_effects) ## Oneway (individual) effect Random Effect Model ## (Swamy-Arora&#39;s transformation) ## ## Call: ## plm(formula = y ~ x1, data = country_panel, model = &quot;random&quot;) ## ## Balanced Panel: n = 7, T = 10, N = 70 ## ## Effects: ## var std.dev share ## idiosyncratic 7.815e+18 2.796e+09 0.873 ## individual 1.133e+18 1.065e+09 0.127 ## theta: 0.3611 ## ## Residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -8.94e+09 -1.51e+09 2.82e+08 0.00e+00 1.56e+09 6.63e+09 ## ## Coefficients: ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept) 1037014329 790626206 1.3116 0.1896 ## x1 1247001710 902145599 1.3823 0.1669 ## ## Total Sum of Squares: 5.6595e+20 ## Residual Sum of Squares: 5.5048e+20 ## R-Squared: 0.02733 ## Adj. R-Squared: 0.013026 ## Chisq: 1.91065 on 1 DF, p-value: 0.16689 Interpretation of the coefficients is a little bit tricky since they include both the within-entity and between-entity effects. In this case, the data represents the average effect of \\(X_{1}\\) over \\(Y\\) when \\(X\\) changes across time and between countries by one unit. If you are not sure whether you should run a fixed effects or a random effects model, you can run a Hausman test to help you with the decision. The Hausman test tests whether the variation across entities (i.e. countries in our example) is uncorrelated with the independent variables. The null hypothesis is that there is no such correlation: if the Hausman specification test comes back significant then it means that you should use a fixed effects model. You run it by comparing the results from both models! phtest(fixed_effects, random_effects) ## ## Hausman Test ## ## data: y ~ x1 ## chisq = 3.674, df = 1, p-value = 0.05527 ## alternative hypothesis: one model is inconsistent According to the Hausman test, we should use the random effects model to estimate the relationship between \\(x_1\\) and our \\(Y\\). Note Normally, this is not the end of it as there is a sequence of tests that can and should be performed to make sure the model is valid. Testing for hetersokedaticity and stochastic trends, for instance. However, this is out of scope of the current module. 2.4 Take home message Different models are used for different research questions: the question guides the model. What is important to keep in mind, especially when working with machine learning models, is that not all models are suitable to say something about the relationships between variables. Let alone on whether variable \\(X\\) is the cause of change in variable \\(Y\\). In this week’s material, we really wanted to introduce you to some models that do explicitly look at relationships between variables from an econometrics point of view - a very tiny sneak peak if you may. Of course, there are many other advanced econometric models such as models using instrumental variables and difference-in-differences models. Inference is difficult but important in social science, and some of these traditional statistical/econometric method try to get more reliable estimates. What we will be learning in the next couple of weeks will be less interpretable/explanable but it is important to keep some of the issues that came to light during this week’s material in mind. That is it for this week! 2.5 Attributions This week’s content and practical uses content and inspiration from: Torres-Reyna, Oscar. 2010. Getting Started in Fixed/Random Effects Models using R. [Link] Huntington-Klein, Nick. 2019. ECON 305: Economics, Causality, and Analytics. Lecture 13: Causality. [Link] "],
["introduction-to-deep-learning.html", "3 Introduction to Deep Learning", " 3 Introduction to Deep Learning This week, we will introduce you to the topic of Deep Learning, its history and its uses in digital humanities, robotics, climate science, geography and social science. We will discuss the principles and algorithms of training and evaluating a deep neural network as an extension of a linear regression model. We will also present the strength and weakness of such techniques. The practical component of the week will allow you to practice and deep learning models, which are highly transferable skills in the future. This week’s material is available on Moodle. "],
["keynote-spatial-data-science.html", "4 Keynote: Spatial Data Science", " 4 Keynote: Spatial Data Science This week we will have a guest lecture from Miguel Alvarez Garcia @Carto on CartoFrame and Spatial Data Science. Miguel is a Data Scientist at CARTO and has experience in optimisation and machine learning. His current work focuses on solving problems with a strong geospatial component in various sectors such as logistics, utilities, retail, and environmental. Details on this guest lecture are available on Moodle. "],
["convolutional-neural-networks.html", "5 Convolutional Neural Networks", " 5 Convolutional Neural Networks This week, we will introduce you to the topic of Convolutional Neural Network, a special case of Neural Network. In particular, we will describe the various CNN architecture and the useful technique of transfer learning. We will discuss the principles behind the different convolutional layers and activation functions. We will also give a case study on street frontage classification and object detection. The practical component of the week will allow you to practice running convolutional neural network models. This week’s material is available on Moodle. "],
["spatial-data-science-applications.html", "6 Spatial Data Science Applications", " 6 Spatial Data Science Applications This week, we will introduce you the topic of applying deep neural networks for spatial data science applications. In particular we will describe the use of multi-modality data in machine learning. In particular we will describe the use of multi-modal data for socio-economic predictions for two different papers. This week’s material is available on Moodle. "],
["spatial-temporal-mobility-analysis.html", "7 Spatial-Temporal Mobility Analysis 7.1 Introduction 7.2 GPS data 7.3 GPS data classification 7.4 Take home message 7.5 Attributions", " 7 Spatial-Temporal Mobility Analysis 7.1 Introduction Against the background of unprecedented growth in private vehicle ownership and the entrenchment of the private car in everyday life, the past decades have seen a growing and ongoing academic and policy debate on how to encourage individuals to change to more sustainable ways of travelling. More recently, researchers have started to build on so-called location-aware technologies, exploring innovative methods to more accurately capture, visualise, and analyse individual spatiotemporal travel patterns: information that can be used to formulate strategies to accommodate the increasing demand for transport vis-à-vis growing environmental and societal concerns. This week we will be looking at capturing mobility data with Global Positioning Systems. We will further use two types of Machine Learning classifiers, specifically Support Vector Machines and tree-based methods, to classify labeled GPS data into stay and move points. This week is structured around three short videos as well as a tutorial in R with a ‘hands-on’ application of GPS data classification. Let’s get to it! Video: Introduction W07 [Lecture slides] [Watch on MS stream] 7.1.1 Reading list Please find the reading list for this week below. Core reading Broach, J. et al.. 2019. Travel mode imputation using GPS and accelerometer data from a multi-day travel survey. Journal of Transport Geography 78: 194-204. [Link] Bohte, W. and K. Maat. 2009. Deriving and validating trip purposes and travel modes for multi-day GPS-based travel surveys: A large-scale application in the Netherlands. Transportation Research Part C: Emerging Technologies 17(3): 285–297. [Link] Feng, T and H. Timmermans. 2016. Comparison of advanced imputation algorithms for detection of transportation mode and activity episode using GPS data. Transportation Planning and Technology 39(2): 180–194. [Link] Nitsche, P. et al.. 2014. Supporting large-scale travel surveys with smartphones - a practical approach. Transportation Research Part C: Emerging Technologies 43: 212–221. [Link] Supplementary reading Behrens, R. et al.. 2006. Collection of passenger travel data in Sub-Saharan African cities: Towards improving survey instruments and procedures. Transport Policy 13: 85-96. [Link] Behrens, R. and Del Mistro, R. 2010. Shocking habits: Methodological issues in analyzing changing personal travel behavior over time. International Journal of Sustainable Transportation 4(5): 253-271. [Link] Van de Coevering, P. et al.. 2021. Causes and effects between attitudes, the built environment and car kilometres: A longitudinal analysis. Journal of Transport Geography 91: 102982. [Link] Van Dijk, J. 2018. Identifying activity-travel points from GPS-data with multiple moving windows. Computers, Environment and Urban Systems 70: 84-101. [Link] Wolf, J. 2000. Using GPS data loggers to replace travel diaries in the collection of travel data. Doctoral dissertation. Atlanta, Georgia: Georgia Institute of Technology. [Link] 7.1.2 Technical Help session Every Thursday between 13h00-14h00 you can join the Technical Help session on Microsoft Teams. The session will be hosted by Alfie. He will be there for the whole hour to answer any question you have live in the meeting or any questions you have formulated beforehand. If you cannot make the meeting, feel free to post the issue you are having in the Technical Help channel on the GEOG0125 Team so that Alfie can help to find a solution. 7.2 GPS data Mobility is a central aspect of everyday life. In its simplest form, human mobility refers to the movement of individuals from location A to location B. This can be a relocation from one city to another city, as well as a trip from home to work. Transport systems provide the physical nodes and linkages that facilitate this mobility. However, transport systems and road networks in many cities around the world are under pressure as a result of unparalleled growth in private vehicle ownership and increasingly complex and fragmented travel patterns. Particularly in urban areas, this is problematic because it leads to problems such as congestion, accidents, road decay, and reduced accessibility. As such, governments and researchers throughout the world have started to recognise the need to curtail demand for private road transport. “Technological breakthroughs [alone] are not going to provide the silver bullet for the mitigation of climate change and energy security threats caused by the transport sector” (Stradling and Anable, 2008: 195) The realisation that increasing road infrastructure and improvements in car technology are not sufficient to address the transport problems around the world has led to the idea that transport planning should shift from supply-side to demand-side passenger transport planning. For this, accurate data are required on individual spatio-temporal behaviour. Travel data collection methods can roughly be classified into two, not per se mutually exclusive, methods. The first method uses self-reported data, such as data collected through telephone-assisted interviews, computer-assisted interviews, and pen-and-paper interviews. The second method relies on passively collected data, such as data collected through call-detail records and GPS data. Technological developments in the field of location-aware technologies, GPS in particular, have greatly enhanced opportunities to collect accurate data on human spatiotemporal behaviour. GPS data need to be collected and analysed systematically to be intelligible for transport researchers and policy makers. Moreover, the challenges inherent to mobile data collection techniques include not only harnessing the tools to obtain geo-referenced data, but also the development of new skills sets for cleaning, analysing, and interpreting these data. Video: GPS data in transport research [Lecture slides] [Watch on MS stream] 7.3 GPS data classification Where GPS technology can precisely register the spatiotemporal elements of activity-travel behaviour, travel characteristics need to be imputed from the data. As such, throughout the last decade or so, a plethora of methods has been developed for identifying trips, activities, and travel modes from raw GPS trajectories. These methods range from deterministic (rule-based) methods to advanced machine learning algorithms. Here, we will focus on using two types of machine learning techniques (Support Vector Machines and tree-based methods) to classify labelled GPS points into stay and move points. Video: GPS data classification with ML techniques [Lecture slides] [Watch on MS stream] The segmentation of GPS data into activity and travel episodes is often the first step in a more elaborate process of identifying activity types and transport modes. A major issue with GPS data imputation, however, is the necessity of a ground truth to test whether the imputation algorithm correctly categorises GPS points into activity (stay) points and trips (move) points. We will be using a set of 50 artificially created GPS trajectories that all contain a sequence of stays and moves in Cape Town, South Africa. For the data that we will use, the artificial GPS ‘records’ a measurement every 60 seconds. To further simulate noise in the data, a random sample comprising of 50 per cent of the data points was taken. Besides these raw GPS data, we also have access to a basic road network lay out of the Mother City. File download File Type Link Cape Town GPS and road data shp Download We will start by downloading the files, importing both into R, and looking at what we will be working with. Be careful not to plot the road network file. Because the road network contains around 120,000 individual road segments it will take a long time to draw. Rather have a look at the data in QGIS, which is much more capable of on the fly displaying a large number of features. # load libraries library(tidyverse) library(sf) library(tmap) # read gps data gps &lt;- read_sf(&#39;raw/w07/gps_cape_town.shp&#39;) # read road data road &lt;- read_sf(&#39;raw/w07/roads_cape_town.shp&#39;) # inspect gps ## Simple feature collection with 27019 features and 9 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 18.34291 ymin: -34.19419 xmax: 19.06105 ymax: -33.66433 ## CRS: 4148 ## # A tibble: 27,019 x 10 ## stop type duration point_id_n timestamp mode track_id move activity ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 MEDI… 30 1 04/02/19… &lt;NA&gt; 001 STAY STAY ## 2 1 MEDI… 30 2 04/02/19… &lt;NA&gt; 001 STAY STAY ## 3 1 MEDI… 30 5 04/02/19… &lt;NA&gt; 001 STAY STAY ## 4 1 MEDI… 30 9 04/02/19… &lt;NA&gt; 001 STAY STAY ## 5 1 MEDI… 30 10 04/02/19… &lt;NA&gt; 001 STAY STAY ## 6 1 MEDI… 30 12 04/02/19… &lt;NA&gt; 001 STAY STAY ## 7 1 MEDI… 30 14 04/02/19… &lt;NA&gt; 001 STAY STAY ## 8 1 MEDI… 30 16 04/02/19… &lt;NA&gt; 001 STAY STAY ## 9 1 MEDI… 30 18 04/02/19… &lt;NA&gt; 001 STAY STAY ## 10 1 MEDI… 30 22 04/02/19… &lt;NA&gt; 001 STAY STAY ## # … with 27,009 more rows, and 1 more variable: geometry &lt;POINT [°]&gt; # inspect road ## Simple feature collection with 125024 features and 1 field ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: 18.34051 ymin: -34.3001 xmax: 19.12007 ymax: -33.62755 ## CRS: 4148 ## # A tibble: 125,024 x 2 ## id geometry ## &lt;dbl&gt; &lt;LINESTRING [°]&gt; ## 1 1 (18.93353 -34.16301, 18.93345 -34.1631, 18.93335 -34.16318, 18.93… ## 2 2 (18.45615 -34.27366, 18.4545 -34.27321, 18.45298 -34.2728, 18.452… ## 3 3 (18.45615 -34.27366, 18.45674 -34.27201, 18.45683 -34.27181, 18.4… ## 4 4 (18.85612 -34.25045, 18.85621 -34.2506, 18.85633 -34.25087, 18.85… ## 5 5 (19.12007 -34.25944, 19.11977 -34.25952, 19.11936 -34.25961, 19.1… ## 6 6 (18.50105 -33.98948, 18.50102 -33.98942, 18.50105 -33.98935, 18.5… ## 7 7 (18.65381 -34.00991, 18.65355 -34.00979, 18.65251 -34.00931, 18.6… ## 8 8 (18.63738 -34.07209, 18.63846 -34.07193, 18.63924 -34.07183, 18.6… ## 9 9 (18.44389 -34.03779, 18.44389 -34.03798, 18.44388 -34.03826, 18.4… ## 10 10 (18.64958 -34.04498, 18.64729 -34.04547, 18.64228 -34.04652, 18.6… ## # … with 125,014 more rows # inspect names(gps) ## [1] &quot;stop&quot; &quot;type&quot; &quot;duration&quot; &quot;point_id_n&quot; &quot;timestamp&quot; ## [6] &quot;mode&quot; &quot;track_id&quot; &quot;move&quot; &quot;activity&quot; &quot;geometry&quot; # project into Hartbeesthoek94 gps &lt;- gps %&gt;% st_transform(crs=&#39;epsg:2053&#39;) road &lt;- road %&gt;% st_transform(crs=&#39;epsg:2053&#39;) # plot gps points tm_shape(gps) + tm_dots() The road network file is relatively simple and only contains road network segments - no additional information (e.g. maximum speed, road type, etc.) is available. The GPS data themselves contain several fields: Column heading Description stop Stop number within GPS trajectory type Type of stop (i.e. short, medium, long) duration Duration of stop in seconds point_id_n Unique point identifier within GPS trajectory timestamp Time the point was recorded mode Travel mode track_id Unique ID for GPS trajectory move Whether point is a move or a stay activity Whether point is a move (including mode) or a stay 7.3.1 GPS data preparation The move variable is the variable we will try to predict. Before we split our data into a train and test set, however, we will need to derive some features from our raw GPS data that we can use as our input features: speed, point density, and distance to nearest road segment. As we want to derive information of consecutive measurements, the order of the data is very important right now as the points form a trajectory: so before we calculate anything we start by making sure that the data are ordered correctly: by track_id and timestamp. # order data so that we do not mix separate trajectories gps &lt;- gps %&gt;% arrange(track_id,timestamp) # calculate distance between consecutive points // could take a few minutes gps &lt;- gps %&gt;% group_by(track_id) %&gt;% mutate( lead_geom = geometry[row_number() + 1], dist = st_distance(lead_geom,geometry, by_element=TRUE, which=&#39;Euclidean&#39;) ) %&gt;% ungroup() # inspect gps ## Simple feature collection with 27019 features and 10 fields ## Active geometry column: geometry ## geometry type: POINT ## dimension: XY ## bbox: xmin: 918250.7 ymin: 3775604 xmax: 986130.2 ymax: 3836086 ## CRS: epsg:2053 ## # A tibble: 27,019 x 12 ## stop type duration point_id_n timestamp mode track_id move activity ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 MEDI… 30 1 04/02/19… &lt;NA&gt; 001 STAY STAY ## 2 1 MEDI… 30 2 04/02/19… &lt;NA&gt; 001 STAY STAY ## 3 1 MEDI… 30 5 04/02/19… &lt;NA&gt; 001 STAY STAY ## 4 1 MEDI… 30 9 04/02/19… &lt;NA&gt; 001 STAY STAY ## 5 1 MEDI… 30 10 04/02/19… &lt;NA&gt; 001 STAY STAY ## 6 1 MEDI… 30 12 04/02/19… &lt;NA&gt; 001 STAY STAY ## 7 1 MEDI… 30 14 04/02/19… &lt;NA&gt; 001 STAY STAY ## 8 1 MEDI… 30 16 04/02/19… &lt;NA&gt; 001 STAY STAY ## 9 1 MEDI… 30 18 04/02/19… &lt;NA&gt; 001 STAY STAY ## 10 1 MEDI… 30 22 04/02/19… &lt;NA&gt; 001 STAY STAY ## # … with 27,009 more rows, and 3 more variables: geometry &lt;POINT [m]&gt;, ## # lead_geom &lt;POINT [m]&gt;, dist [m] # calculate time between consecutive points gps &lt;- gps %&gt;% group_by(track_id) %&gt;% mutate( lead_time = strptime(timestamp[row_number() + 1], format=&#39;%d/%m/%Y %H:%M:%S&#39;), diff = difftime(lead_time,strptime(timestamp, format=&#39;%d/%m/%Y %H:%M:%S&#39;), units=&#39;secs&#39;) ) %&gt;% ungroup() # inspect gps ## Simple feature collection with 27019 features and 12 fields ## Active geometry column: geometry ## geometry type: POINT ## dimension: XY ## bbox: xmin: 918250.7 ymin: 3775604 xmax: 986130.2 ymax: 3836086 ## CRS: epsg:2053 ## # A tibble: 27,019 x 14 ## stop type duration point_id_n timestamp mode track_id move activity ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 MEDI… 30 1 04/02/19… &lt;NA&gt; 001 STAY STAY ## 2 1 MEDI… 30 2 04/02/19… &lt;NA&gt; 001 STAY STAY ## 3 1 MEDI… 30 5 04/02/19… &lt;NA&gt; 001 STAY STAY ## 4 1 MEDI… 30 9 04/02/19… &lt;NA&gt; 001 STAY STAY ## 5 1 MEDI… 30 10 04/02/19… &lt;NA&gt; 001 STAY STAY ## 6 1 MEDI… 30 12 04/02/19… &lt;NA&gt; 001 STAY STAY ## 7 1 MEDI… 30 14 04/02/19… &lt;NA&gt; 001 STAY STAY ## 8 1 MEDI… 30 16 04/02/19… &lt;NA&gt; 001 STAY STAY ## 9 1 MEDI… 30 18 04/02/19… &lt;NA&gt; 001 STAY STAY ## 10 1 MEDI… 30 22 04/02/19… &lt;NA&gt; 001 STAY STAY ## # … with 27,009 more rows, and 5 more variables: geometry &lt;POINT [m]&gt;, ## # lead_geom &lt;POINT [m]&gt;, dist [m], lead_time &lt;dttm&gt;, diff &lt;drtn&gt; # calculate speed in kilometres per hour gps &lt;- gps %&gt;% mutate(speed = as.integer(dist)/as.integer(diff)*3.6) # inspect gps ## Simple feature collection with 27019 features and 13 fields ## Active geometry column: geometry ## geometry type: POINT ## dimension: XY ## bbox: xmin: 918250.7 ymin: 3775604 xmax: 986130.2 ymax: 3836086 ## CRS: epsg:2053 ## # A tibble: 27,019 x 15 ## stop type duration point_id_n timestamp mode track_id move activity ## * &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 MEDI… 30 1 04/02/19… &lt;NA&gt; 001 STAY STAY ## 2 1 MEDI… 30 2 04/02/19… &lt;NA&gt; 001 STAY STAY ## 3 1 MEDI… 30 5 04/02/19… &lt;NA&gt; 001 STAY STAY ## 4 1 MEDI… 30 9 04/02/19… &lt;NA&gt; 001 STAY STAY ## 5 1 MEDI… 30 10 04/02/19… &lt;NA&gt; 001 STAY STAY ## 6 1 MEDI… 30 12 04/02/19… &lt;NA&gt; 001 STAY STAY ## 7 1 MEDI… 30 14 04/02/19… &lt;NA&gt; 001 STAY STAY ## 8 1 MEDI… 30 16 04/02/19… &lt;NA&gt; 001 STAY STAY ## 9 1 MEDI… 30 18 04/02/19… &lt;NA&gt; 001 STAY STAY ## 10 1 MEDI… 30 22 04/02/19… &lt;NA&gt; 001 STAY STAY ## # … with 27,009 more rows, and 6 more variables: geometry &lt;POINT [m]&gt;, ## # lead_geom &lt;POINT [m]&gt;, dist [m], lead_time &lt;dttm&gt;, diff &lt;drtn&gt;, ## # speed &lt;dbl&gt; Because we are using consecutive rows to calculate our speed, all of the last measurements within each of the 50 trajectories will not be assigned a value. However, because we do not really want to throw these points out, we will simply assign these points the same speed value as the last point that did get a value assigned. # fill missing speeds gps &lt;- gps %&gt;% fill(speed, .direction=&#39;down&#39;) Another useful input feature would be a local point density: for each point it would be useful to know how many other points are in its vicinity, for instance, because a clustering of points could be indicative of an activity. Because vicinity is difficult to define, we will use three distance thresholds to create these local point densities: 100m, 250m, and 500m. # distance thresholds gps_100m &lt;- st_buffer(gps,100) gps_250m &lt;- st_buffer(gps,250) gps_500m &lt;- st_buffer(gps,500) # loop through trajectories and count the number of points falling within the three distance thresholds # // this could take a few minutes df &lt;- gps[0,] for (t in seq(1,50)) { # filter gps trajectory gps_sel &lt;- filter(gps, as.integer(track_id)==t) # filter buffer gps_100m_sel &lt;- filter(gps_100m, as.integer(track_id)==t) gps_250m_sel &lt;- filter(gps_250m, as.integer(track_id)==t) gps_500m_sel &lt;- filter(gps_500m, as.integer(track_id)==t) # intersect gps_sel$buf100 &lt;- lengths(st_intersects(gps_sel, gps_100m_sel)) gps_sel$buf250 &lt;- lengths(st_intersects(gps_sel, gps_250m_sel)) gps_sel$buf500 &lt;- lengths(st_intersects(gps_sel, gps_500m_sel)) # bind results df &lt;- rbind(df,gps_sel) } # rename gps &lt;- df # inspect gps ## Simple feature collection with 27019 features and 16 fields ## Active geometry column: geometry ## geometry type: POINT ## dimension: XY ## bbox: xmin: 918250.7 ymin: 3775604 xmax: 986130.2 ymax: 3836086 ## CRS: epsg:2053 ## # A tibble: 27,019 x 18 ## stop type duration point_id_n timestamp mode track_id move activity ## * &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 MEDI… 30 1 04/02/19… &lt;NA&gt; 001 STAY STAY ## 2 1 MEDI… 30 2 04/02/19… &lt;NA&gt; 001 STAY STAY ## 3 1 MEDI… 30 5 04/02/19… &lt;NA&gt; 001 STAY STAY ## 4 1 MEDI… 30 9 04/02/19… &lt;NA&gt; 001 STAY STAY ## 5 1 MEDI… 30 10 04/02/19… &lt;NA&gt; 001 STAY STAY ## 6 1 MEDI… 30 12 04/02/19… &lt;NA&gt; 001 STAY STAY ## 7 1 MEDI… 30 14 04/02/19… &lt;NA&gt; 001 STAY STAY ## 8 1 MEDI… 30 16 04/02/19… &lt;NA&gt; 001 STAY STAY ## 9 1 MEDI… 30 18 04/02/19… &lt;NA&gt; 001 STAY STAY ## 10 1 MEDI… 30 22 04/02/19… &lt;NA&gt; 001 STAY STAY ## # … with 27,009 more rows, and 9 more variables: geometry &lt;POINT [m]&gt;, ## # lead_geom &lt;POINT [m]&gt;, dist [m], lead_time &lt;dttm&gt;, diff &lt;drtn&gt;, ## # speed &lt;dbl&gt;, buf100 &lt;int&gt;, buf250 &lt;int&gt;, buf500 &lt;int&gt; Great. We now have a speed variable as well as three local density variables. The last thing we will need to do is for each point calculate the distance to the nearest road segment. # for each point: get nearest road feature # // this could take a few minutes nearest_road &lt;- st_nearest_feature(gps, road) # for each point: get the distance to the nearest road feature # // this could take a few minutes nearest_road_dst &lt;- st_distance(gps, road[nearest_road,], by_element=TRUE, which=&#39;Euclidean&#39;) # assign values to gps data set gps$road_dist &lt;- nearest_road_dst # inspect gps ## Simple feature collection with 27019 features and 17 fields ## Active geometry column: geometry ## geometry type: POINT ## dimension: XY ## bbox: xmin: 918250.7 ymin: 3775604 xmax: 986130.2 ymax: 3836086 ## CRS: epsg:2053 ## # A tibble: 27,019 x 19 ## stop type duration point_id_n timestamp mode track_id move activity ## * &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 MEDI… 30 1 04/02/19… &lt;NA&gt; 001 STAY STAY ## 2 1 MEDI… 30 2 04/02/19… &lt;NA&gt; 001 STAY STAY ## 3 1 MEDI… 30 5 04/02/19… &lt;NA&gt; 001 STAY STAY ## 4 1 MEDI… 30 9 04/02/19… &lt;NA&gt; 001 STAY STAY ## 5 1 MEDI… 30 10 04/02/19… &lt;NA&gt; 001 STAY STAY ## 6 1 MEDI… 30 12 04/02/19… &lt;NA&gt; 001 STAY STAY ## 7 1 MEDI… 30 14 04/02/19… &lt;NA&gt; 001 STAY STAY ## 8 1 MEDI… 30 16 04/02/19… &lt;NA&gt; 001 STAY STAY ## 9 1 MEDI… 30 18 04/02/19… &lt;NA&gt; 001 STAY STAY ## 10 1 MEDI… 30 22 04/02/19… &lt;NA&gt; 001 STAY STAY ## # … with 27,009 more rows, and 10 more variables: geometry &lt;POINT [m]&gt;, ## # lead_geom &lt;POINT [m]&gt;, dist [m], lead_time &lt;dttm&gt;, diff &lt;drtn&gt;, ## # speed &lt;dbl&gt;, buf100 &lt;int&gt;, buf250 &lt;int&gt;, buf500 &lt;int&gt;, road_dist [m] 7.3.2 GPS data classification Now we have added some useful variables to our raw GPS trajectories, we can scale them and move on to our classification algorithms. We will use a C5.0 boosted decision tree, a random forest, and a support vector machine. We will use a test/train split of 70/30. # libraries library(e1071) library(randomForest) library(C50) library(caret) # assign point to train and test gps &lt;- gps %&gt;% mutate(train = if_else(runif(nrow(gps)) &lt; 0.7, 1, 0)) # create train set, select variables, confirm data types where necessary gps_train &lt;- gps %&gt;% filter(train==1) %&gt;% select(speed,buf100,buf250,buf500,road_dist,move) %&gt;% mutate(road_dist=as.numeric(road_dist)) # drop geometry gps_train &lt;- st_drop_geometry(gps_train) # scale gps_train[1:5] &lt;- lapply(gps_train[1:5], function(x) scale(x)) # inspect gps_train ## # A tibble: 18,943 x 6 ## speed[,1] buf100[,1] buf250[,1] buf500[,1] road_dist[,1] move ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 -0.734 -0.474 -0.755 -0.896 -0.742 STAY ## 2 -0.516 -0.452 -0.755 -0.896 -0.240 STAY ## 3 -0.594 -0.563 -0.770 -0.896 -0.316 STAY ## 4 -0.409 -0.519 -0.755 -0.896 0.161 STAY ## 5 -0.450 -0.541 -0.770 -0.896 -0.658 STAY ## 6 -0.622 -0.408 -0.755 -0.896 -0.271 STAY ## 7 -0.560 -0.541 -0.755 -0.896 1.75 STAY ## 8 -0.571 -0.496 -0.755 -0.896 1.05 STAY ## 9 -0.740 -0.474 -0.755 -0.896 -0.533 STAY ## 10 2.49 -0.430 -0.755 -0.896 -0.681 STAY ## # … with 18,933 more rows # create train set, select variables, confirm data types where necessary gps_test &lt;- gps %&gt;% filter(train==0) %&gt;% select(speed,buf100,buf250,buf500,road_dist,move) %&gt;% mutate(road_dist=as.numeric(road_dist)) # drop geometry gps_test &lt;- st_drop_geometry(gps_test) # scale gps_test[1:5] &lt;- lapply(gps_test[1:5], function(x) scale(x)) # inspect gps_test ## # A tibble: 8,076 x 6 ## speed[,1] buf100[,1] buf250[,1] buf500[,1] road_dist[,1] move ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 -0.682 -0.501 -0.760 -0.902 -0.241 STAY ## 2 -0.586 -0.567 -0.760 -0.902 1.44 STAY ## 3 -0.641 -0.478 -0.760 -0.902 -0.209 STAY ## 4 4.83 -0.612 -0.938 -1.04 -0.547 MOVE ## 5 5.22 -0.589 -0.923 -1.03 -0.391 MOVE ## 6 4.83 -0.612 -0.938 -1.04 -0.698 MOVE ## 7 5.37 -0.612 -0.938 -1.04 -0.408 MOVE ## 8 -0.714 -0.545 -0.686 -0.772 -0.625 STAY ## 9 -0.722 -0.434 -0.597 -0.772 0.563 STAY ## 10 -0.467 -0.301 -0.597 -0.772 0.475 STAY ## # … with 8,066 more rows Let’s train our models on our train data and directly predict on our test data. # boosted decision tree with 5 boosting iterations train_boost &lt;- C5.0(as.factor(move) ~., data=gps_train, trials=5) # support vector machine train_svm &lt;- svm(as.factor(move) ~ ., data=gps_train) # random forest with 500 trees, 3 variables at each split, sampling with replacement train_rf &lt;- randomForest(as.factor(move) ~ ., data=gps_train, ntree=500, replace=TRUE, mtry=3) # predict boosted decision tree test_boost &lt;- predict(train_boost, gps_test) # predict support vector machine test_svm &lt;- predict(train_svm, gps_test) # predict random forest test_rf &lt;- predict(train_rf, gps_test) We will use a confusion matrix to inspect our results. A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the actual values are known. In other words, a confusion matrix can be used to compare the predictions our models make to the ground truth. A confusion matrix basically informs you for all prediction classes how many data points where predicted correctly and how many data points were predicted incorrectly. # create confusion matrices matrix_boost &lt;-table(test_boost, gps_test$move) matrix_svm &lt;- table(test_svm, gps_test$move) matrix_rf &lt;- table(test_rf, gps_test$move) # inspect matrix_boost ## ## test_boost MOVE STAY ## MOVE 3175 128 ## STAY 125 4648 # inspect matrix_svm ## ## test_svm MOVE STAY ## MOVE 3184 158 ## STAY 116 4618 # inspect matrix_rf ## ## test_rf MOVE STAY ## MOVE 3155 77 ## STAY 145 4699 # get overall accuracy boosted decision tree confusionMatrix(matrix_boost)$overall ## Accuracy Kappa AccuracyLower AccuracyUpper AccuracyNull ## 0.9686726 0.9351892 0.9646383 0.9723639 0.5913819 ## AccuracyPValue McnemarPValue ## 0.0000000 0.8999386 # get overall accuracy support vector machine confusionMatrix(matrix_svm)$overall ## Accuracy Kappa AccuracyLower AccuracyUpper AccuracyNull ## 0.96607231 0.92993754 0.96188997 0.96991376 0.59138187 ## AccuracyPValue McnemarPValue ## 0.00000000 0.01325288 # get overall accuracy random forest confusionMatrix(matrix_rf)$overall ## Accuracy Kappa AccuracyLower AccuracyUpper AccuracyNull ## 9.725111e-01 9.429407e-01 9.687083e-01 9.759677e-01 5.913819e-01 ## AccuracyPValue McnemarPValue ## 0.000000e+00 6.900148e-06 As we can see, all algorithms are highly accurate in classifying our artificial GPS points into moves and stays using the input features that were derived from the raw GPS trajectory data. 7.3.3 Exercise Now we have worked through classifying our points into moves and stays using raw GPS points, there are three further exercises that we want you to do: Add a new set of variables that, for every point in the dataset, contains the number of points within a distance of 100m, 250m, and 500m but for only those points that are within 5 minutes (both sides) of the point under consideration (i.e. use a moving time window to select the points that qualify). So, for instance, for point \\(x\\) 10 other points are within a distance of 100m but only 5 of these points were recorded within 5 minutes of point \\(x\\): we now only want those 5 points and not the full 10 points. Please note: this is not a trivial task and you will have to considerably change the code that you have used so far. Instead of using the move column, use the activity column to classify the GPS points into travel modes (i.e. walk, bike , car) and stays. Incorporate all existing variables as well as the three variables you created in Exercise 1. Assess the relative importance of each of the input variables used in Exercise 2 by permutating (that is shuffling) each input variable and re-running the train and test sequence. So, for instance, shuffle the values within the speed column but leaving all the other values untouched, train the three models, and look at the accuracy and Kappa values to see the new results. Repeat this process for all columns in which every time one of the columns gets permutated but all the other values are untouched. With every iteration you save the accuracy and Kappa values so to get an idea about each variable’s relative importance: the variable that causes the largest drop in accuracy and Kappa values is relatively the most important one. Tip 1: You can shuffle your data by sampling without replacement, e.g. gps_test$speed &lt;- sample(gps_test$speed). Tip 2: Create a function or a loop to conduct this process! 7.4 Take home message Where GPS technology can precisely register locational information, travel characteristics need to be imputed from the data before it can be used by transport researchers and policy makers. As such, throughout the last decade, various methods have been developed for identifying trips, activities, and travel modes from raw GPS trajectories. In this week’s material, we took a closer look at GPS data. We also introduced you to three different discriminative classifiers to classify GPS points into moves and stays. Be aware that we only given you a brief introduction to these classifiers here and there are in fact many different implementations of decision trees as well many ways of parameterising support vector machines (e.g. choice of kernel). It is also important to keep in mind that because the GPS points used in the tutorial were artificially created, the models results in relatively high prediction rates and will be very difficult to transfer to a different context. While this is obviously an issue, the advantage of using artificial data is that parameters and noise levels can be precisely tuned which allow you to systematically compare, test, and develop different algorithms. 7.5 Attributions This week’s content and practical uses content and inspiration from: Van Dijk, J. 2017. Designing Travel Behaviour Change Interventions: a spatiotemporal perspective. Doctoral dissertation. Stellenbosch: Stellenbosch University. [Link] Van Dijk, J. 2018. Identifying activity-travel points from GPS-data with multiple moving windows. Computers, Environment and Urban Systems 70: 84-101. [Link] "],
["unix-tools.html", "8 Unix Tools 8.1 Introduction 8.2 Unix Shell 8.3 Bourne Again SHell (BASH) 8.4 Text editing 8.5 Data wrangling 8.6 Copying data 8.7 Web 8.8 Take home message 8.9 Attributions", " 8 Unix Tools 8.1 Introduction Over the past months, you have learned all about advanced topics in the domains of spatial analysis and machine learning. However, in your career as a data scientist or researcher you are also very likely to encounter having to work in a command line environment. For instance, because you need to run your code or analysis on a dedicated high-performance machine or because you have created some maps that you want to host on a website. To do this, and to take full advantage of the tools that your computer provides, this week we will be working with a text interface: the Shell. A shell is a computer program which exposes an operating system’s services to a human user or other program. Although several shell programmes exist, at their core they are all roughly the same: they allow you to run programmes from within a textual environment. This week we will focus on one of the most widely used shells: bash (Bourne Again SHell). We will look into accessing a remote server, using some basic commands and programmes, and using some Unix tools for basic data wrangling tasks. The lecture videos this week are provided by Dr Balamurugan Soundararaj. Bala obtained a Masters’ degree from CASA and a PhD from UCL Geography. Moreover, Bala is an avid Unix user and he has made extensive use of Unix tools to set up a data processing pipeline for his PhD research. Let’s get started! Video: Introduction W08 [Lecture slides] [Watch on MS stream] 8.1.1 Reading list Because this week’s tutorial material is rather extensive and has a strong practical focus, we only have one article as suggested reading this week. Suggested reading Soundararaj, B. et al.. 2019. Medium Data Toolkit-A Case Study on Smart Street Sensor Project. Proceedings of the 27th Conference of GIS Research UK (GISRUK). Newcastle: Newcastle University. [Link] 8.1.2 Technical Help session Every Thursday between 13h00-14h00 you can join the Technical Help session on Microsoft Teams. The session will be hosted by Alfie. He will be there for the whole hour to answer any question you have live in the meeting or any questions you have formulated beforehand. If you cannot make the meeting, feel free to post the issue you are having in the Technical Help channel on the GEOG0125 Team so that Alfie can help to find a solution. 8.2 Unix Shell Before we get started with the Unix Shell, it would be very useful to know where different programming languages stand in the grand scheme of things. As such, in the first short lecture video Bala will give a brief introduction to computer science from a non-computer scientist’s perspective. Video: Introduction to Computer Sciences [Lecture slides] [Demo data] [Watch on MS stream] Now we have had a basic introduction to computer science from a non-computer scientist’s perspective, we can dive into the Unix Shell. Video: Introduction to Unix Shell [Lecture slides] [Watch on MS stream] 8.3 Bourne Again SHell (BASH) Bash is a Unix shell and command language. It is free software and has been distributed widely as the default login shell for most Linux distributions and Apple’s macOS, a version is also available for Windows. To open a shell prompt, which is where you can type commands, you first need a terminal. 8.3.1 Installation Windows Download the Git for Windows installer. It depends on your machine, but you probably need the Git-2.30.1-64-bit.exe version. Run the installer and follow the steps below: Click on next four times (two times if you’ve previously installed Git). You don’t need to change anything in the “information”, “location”, “components”, and “start menu” screens. From the dropdown menu select “Use Vim (the ubiquitous text editor as Git’s default editor”. Click next. On the page that says “Adjusting the name of the initial branch in new repositories”, leave the default as “Let Git decide”. Click next. Leave the\" “Git from the command line and also from 3rd-party software” button selected and click on next. Ensure that “Use the native Windows Secure Channel Library” is selected and click on next. Ensure that “Checkout Windows-style, commit Unix-style line endings” is selected and click on next. Ensure that “Use Windows’ default console window” is selected and click on next. Ensure that \"Default (fast-forward or merge) is selected and click next. Ensure that “Do not use a credential help” is selected and click on next. Ensure that “Enable file system caching” is selected and click on next. Ensure that “Enable experimental support for pseudo consoles” is not selected (default) and click on install. Click on finish or next. If your HOME environment variable is not set (or you don’t know what this is): open command prompt (Open Start Menu then type cmd and press Enter) Type the following line into the command prompt window exactly as shown: setx HOME \"%USERPROFILE%\" Press enter. You should see: SUCCESS: Specified value was saved. Quit command prompt by typing exit then pressing enter. You can now open the programme Git Bash. macOS The default shell in some versions of macOS is Bash, and Bash is available in all versions, so no need to install anything. You access Bash from the Terminal (found in /Applications/Utilities). Linux The default shell is usually Bash and there is usually no need to install anything. If you are running a distribution like Ubuntu Desktop you can also access Bash from the Terminal. 8.3.2 Getting started Now everyone should have access to a terminal, either by installing Git Bash or directly through your operating system, we can get started. In order to make sure that everyone has access to the same tools and functionality, we will use our terminal to log into a server that we have created specifically for this course. In the following, we will log in to this server through something called ssh (Secure Shell). The SSH protocol uses encryption to secure the connection between a client and a server. Start your terminal and type in the following: ssh username@178.79.152.249 Note You can find your username and password on Moodle. When you log in for the first time, you will get a message that The authenticity of host 178.79.152.249 can't be established together with the question whether you want to continue connecting. Simply type yes and hit enter to log on to the server. You terminal window should now look similar to Figure 8.1. Figure 8.1: Enter the Matrix. Now we have successfully logged onto the server, you will see a prompt that looks like this: username@geog:~$. This is the main textual interface to the shell. It tells you that you are on the machine geog and that your “current working directory”, or where you currently are, is ~ (short for “home”). The $ tells you that you are not the root user (the root user can do many things an ordinary user cannot). At this prompt you can type a command, which will then be interpreted by the shell. The most basic command is to execute a program: date ## Sat Mar 6 12:49:27 GMT 2021 Here, we executed the date program, which (perhaps unsurprisingly) prints the current date and time. The shell then asks us for another command to execute. We can also execute a command with arguments: echo hello ## hello In this case, we told the shell to execute the program echo with the argument “hello”. The echo program simply prints out its arguments, similar to print() in R or Python. Note The shell parses the command by splitting it by whitespace, and then runs the program indicated by the first word, supplying each subsequent word as an argument that the program can access. If you want to provide an argument that contains spaces or other special characters (e.g., a directory named “My Documents”), you can either quote the argument with ' or \" (\"My Documents\"), or escape the relevant characters with \\ (My\\\\ Documents). But how does the shell know how to find the date or echo programmes? The shell is a programming environment, just like Python or R, and so it has variables, conditionals, loops, and functions. When you run commands in your shell, you are really writing a small bit of code that your shell interprets. If the shell is asked to execute a command that does not match one of its programming keywords, it consults an environment variable called $PATH that lists which directories the shell should search for programmes when it is given a command: echo $PATH ## /Applications/IMOD/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/opt/X11/bin:/Applications/IMOD/bin:/opt/miniconda3/bin:/opt/miniconda3/condabin When we run the echo command, the shell sees that it should execute the program echo, and then searches through the :-separated list of directories in $PATH for a file by that name. When it finds it, it runs it. We can find out which file is executed for a given program name using the which program. We can also bypass $PATH entirely by giving the path to the file we want to execute. You may have encountered working with this $PATH variable already when you are using miniconda to manage your virtual environments - conda has to be added to the PATH variable to be available; e.g. in case of conda env list the shell will find conda by consulting the PATH variable and in case this has not been set up properly you will get a conda: command not found message. A (file) path on the shell is a delimited list of directories; separated by / on Linux and macOS and \\ on Windows. On Linux and macOS, the path / is the “root” of the file system, under which all directories and files lie, whereas on Windows there is one root for each disk partition (e.g., C:\\). Because we are working on the GEOG0125 Linux server we will use / for the remainder of this tutorial. A path that starts with / is called an absolute path. Any other path is a relative path. Relative paths are relative to the current working directory, which we can see with the pwd command and change with the cd command. Again, this is not very different than how you deal with paths in R and Python. In a path, . refers to the current directory, and .. to its parent directory. To see what lives in a given directory, we use the ls command. Try out out these commands to navigate the server. Figure 8.2: Navigating in the shell. Unless a directory is given as its first argument, ls will print the contents of the current directory. Most commands accept flags and options (flags with values) that start with - to modify their behaviour. Usually, running a program with the -h or --help flag will print some help text that tells you what flags and options are available. For example, ls --help tells us: that the -l flag uses a long listing format and gives us additional information about each file or directory present. Figure 8.3: Using the -l flag with ls. In the shell, programmes have two primary “streams” associated with them: their input stream and their output stream. When the program tries to read input, it reads from the input stream, and when it prints something, it prints to its output stream. Normally, a program’s input and output are both your terminal. That is, your keyboard as input and your screen as output. However, we can also rewire those streams. The simplest form of redirection is &lt; file and &gt; file. These let you rewrite the input (“standard input”) and output (“standard output”) streams of a programme to a file respectively. Navigate to your home directory (i.e. /home/username) and try the following: # redirect echo to file echo hello &gt; hello.txt # output content of file to terminal cat hello.txt # output content of file to terminal cat &lt; hello.txt You can also use &gt;&gt; to append to a file. Where this kind of input/output redirection really shines is in the use of pipes. The | operator lets you chain programmes such that the output of one is the input of another. We will come back to this later. For now, some other useful commands are: command action ls list directory contents echo print text to the terminal window touch creates a file mkdir creates a directory grep search text for patterns man print manual or get help for a command pwd print working directory mv move or rename file or directory head read the start of a file tail read the end of a file cat concatenate file (e.g. view file contents history list your most recent commands clear clear your terminal window cp copy files or directory rm remove a file Note Be very careful with using the rm file: if you use rm to delete a file the file will be gone completely and you cannot “recover it from the trash” because there is no trash can for rm! 8.3.3 Exercise Try to do the following, using the man programme or --help flag when you are stuck: Create a new directory called data in your home directory (/home/username). Use touch to create a new file called data.txt. Add three lines of data to data.txt; it does not matter what you add to the file!. Inspect the file with head and tail. Rename the file to data_renamed.txt. 8.4 Text editing When working in a server environment, you will spend some time reading, navigating, and editing code, so it is worthwhile to spend a little attention to text editors. Programmers have very strong opinions about their text editors, but one of the most popular command-line-based text editor is Vim. Vim has a rich history; it originated from the Vi editor (1976), and it is still being developed today. When programming, you spend most of your time reading/editing, not writing. For this reason, Vim is a modal editor: it has different modes for inserting text and manipulating text. The following operating modes are available: Normal: for moving around a file and making edits Insert: for inserting text Replace: for replacing text Visual: for selecting blocks of text Command-line: for running a command Keystrokes have different meanings in different operating modes. For example, the letter x in Insert mode will just insert a literal character “x”, but in Normal mode, it will delete the character under the cursor, and in Visual mode, it will delete the selection. In its default configuration, Vim shows the current mode in the bottom left. The initial/default mode is Normal mode. The usage of different modes and navigating around Vim can be a little difficult in the beginning. The question “how do I exit the Vim editor” has over 4,000 upvotes on stackoverflow! You change modes by pressing ESC (the escape key) to switch from any mode back to Normal mode. From Normal mode, enter Insert mode with i, Replace mode with R, Visual mode with v, Visual Line mode with V, Visual Block mode with ctrl + v, and Command-line mode with :. Inserting text From Normal mode, press i to enter Insert mode. Now, Vim behaves like any other text editor, until you press to return to Normal mode. This, along with the basics explained above, are all you need to start editing files using Vim. Buffers, tabs, and windows Vim maintains a set of open files, called buffers. A Vim session has a number of tabs, each of which has a number of windows (split panes). Each window shows a single buffer. Unlike other programmes you are familiar with, like web browsers, there is not a 1-to-1 correspondence between buffers and windows; windows are merely views. A given buffer may be open in multiple windows, even within the same tab. This can be quite handy, for example, to view two different parts of a file at the same time. By default, Vim opens with a single tab, which contains a single window. Command-line Command mode can be entered by typing : in Normal mode. Your cursor will jump to the command line at the bottom of the screen upon pressing :. This mode has many functionalities, including opening, saving, and closing files, and quitting Vim. command action :q quit (close Vim) :w write (save) :wq write (save file) and quit Vim :e {*name of file*} open file for editing :ls show open buffers Movement You should spend most of your time in Normal mode, using movement commands to navigate the buffer. Some useful movement commands: command action hjkl basic movement (left, down, up, right) w next word b beginning of word e end of word 0 beginning of line $ end of line H top of screen M middle of screen L bottom of screen ctrl + u scroll up ctrl + d scroll down Selection From Normal mode, press v to enter Visual mode, V to enter Visual Line mode, and ctrl + v to enter Visual Block mode. You can use movement keys to select text. Edits For some more advanced editing, i.e. outside of the Insert mode i, some useful commands are: command action o insert line below O insert line above dw delete word d$ delete till end of line d0 delete to beginning of line x delete character s subsitute character u undo y yank (copy) p paste Customising Vim Vim is customisable through a plain-text configuration file in ~/.vimrc (containing Vimscript commands) and there are also tons of plugins for extending Vim, however, this is out of the scope of this tutorial. Interesting fact: Dr Soundararaj actually used his customed Vim installation to write his entire PhD thesis. Video: MIT’s Missing Semester: Vim For a more in-depth introduction to Vim than provided here, do have a look at the lecture video that is part of MIT’s Missing Semester course on which lots of this week’s material is based. 8.4.1 Exercise Vim has a built-in tutorial (vimtutor) that comes installed with Vim. You can start it simply by typing vimtutor in your shell. Complete the vimtutor to get comfortable moving around Vim. You do not need to become an expert in Vim, unless you want to, but just be comfortable enough to access and edits files. Figure 8.4: Vimtutor. 8.5 Data wrangling Now we know how to navigate the terminal as well as view and edit files, we can have a look at some basic data wrangling tasks. Earlier we mentioned that input/output redirection is really useful when using pipes (the | operator) because it allows you to chain programmes such that the output of one is the input of the other. Let’s move to the data directory we created earlier (home/username/data) and try this out with a very simple example. # move to your data directory cd /home/username/data # copy file &quot;file_a&quot; to your data directory cp /home/justin/data/file_a ./ # copy file &quot;file_b&quot; to your data directory cp /home/justin/data/file_b ./ # inspect head file_a file_b Both files contain address references. Although these are artificial data, these address references could, for instance, have been created during an address-matching procedure where raw address strings have been matched to a database containing georeferenced address strings. For this example: let’s say that we tried running some address matching code two times with slightly different parameters on the same input dataset. file_a and file_b contain the address references of all processed addresses. Unfortunately, whilst everything worked fine during our first run (file_a), our second run (file_b) did not completely finish. We now want to know whether addresss reference a46034025 is part of our results. We also want to figure out which of the address records did not get processed so we can restart our address matching procedure without having to rerun the analysis on the full dataset again. # count the number of lines in file a wc -l file_a # count the number of lines in file b wc -l file_b ## 5000 file_a ## 3200 file_b When quickly inspecting the number of lines using wc -l, we can indeed see that the file_a contains more values than file_b. To filter out data, and see if address reference a46034025 is present in both files we can use the grep programme # grep a46034025 grep a46034025 file_a # grep a46034025 grep a46034025 file_b ## a46034025 ## a46034025 Now, let’s try to filter file_a using file_b. Any idea what the flags (-Fxvf) do that have been provided to grep? # get difference grep -Fxvf file_b file_a &gt; file_c # inspect head file_c # inspect wc -l file_c ## a100023196539 ## a100051944010 ## a100021498761 ## a100110679535 ## a10013654653 ## a100030211155 ## a100110100406 ## a119013639 ## a100061891790 ## a10091164372 ## 1800 file_c But what if we only want to now the differences in number of lines within each file? Let’s try if we can combine grep with wc -l. # get difference grep -Fxvf file_b file_a | wc -l ## 1800 In this example the output stream from grep (i.e. the addresses that are not present in file_a) serve as input to wc -l. This may seem trivial, however, this is very powerful. You could think of a situation where you go through a file, perform a database query, analyse the results of this query in a Python script, and push these analysis result to an R script to create a set of visualisations. You can also combine these pipelines in a bash scripts (*.sh). Anything you can run normally on the command line can be put into a script and it will do exactly the same thing. Similarly, anything you can put into a script can also be run normally on the command line and it will do exactly the same thing. In the final lecture video of this week, Bala will show an example of a bespoke processing pipepline that he developed in his research. Video: Medium Data Toolkit [Lecture slides] [Watch on MS stream] 8.6 Copying data Sometimes you will need to upload files to the server. For instance, you have created an analysis pipeline on your local machine and you know want to move this code to a high-performance machine for parallel processing. You can do this by making use of the secure copy protocol (SCP). Secure copy protocol (SCP) is a means of securely transferring computer files between a local host and a remote host or between two remote hosts - and this can be done through with the scp programme. In your terminal: navigate on your local machine (local host) to the file you want to copy to the server (remote host) and then use the following syntax to copy the file to your home directory: # scp from local to remote host scp file_you_want_to_copy username@178.79.152.249:/home/username/ The exact location to which the source file will be copied is specified by username@178.79.152.249:/home/username/, which includes: the name of the account on the destination computer (username); the hostname of the computer to which the source file will be copied (178.79.152.249); and, the directory to which the source fill will be copied (/home/username). Give it a try with any small text file that you may have on your own computer! Note Make sure to include a space between the source and destination paths. Also, be careful when copying files that share the same name on both hosts; you may accidentally overwrite data you intended to keep. 8.7 Web You may have noticed that in your home folder (/home/username) there is a folder called web which contains a file called index.html. We actually have set up the GEOG0125 server as a web server, meaning that the server is able to host some lightweight HTML pages. If you navigate to http://178.79.152.249/justin/ in your web browser you will actually notice that your individual index.html page is available through as well. Any changes you make in the index.html file (e.g. by updating or replacing the file) will automatically reflect on your personal page. updating or replacing the index.html page in your home folder will be reflected in your browser. Your browser will display a notice when navigating to the GEOG125 server that the connection is not secure. This is for our purposes during this course nothing to worry about. The reason for this happening is because the server has not been set up with a SSL certificate. SSL certificates are a small data files that establish an encrypted link between a web server and a browser. This link ensures that all data passed between the web server and browser remain private. Because our server does not have a domain name associated (e.g. something like www.geog0125.co.uk) and can only be accessed by its IP address, we could not issue a certificate using the nonprofit Certificate Authority Let’s Encrypt. Assignment HTML is the foundation of all web pages. It defines the structure of a page, while CSS defines its style. In anticipation of next week’s material where we will be looking at creating a web map using JavaScript and Leaflet.js, have a look at this HTML tutorial: https://developer.mozilla.org/en-US/docs/Learn. From the Getting started with the Web material, we would recommend that at an absolute minimum you go through: Dealing with files HTML basics CSS basics We further would recommend that you have a look at some of the HTML — Structuring the Web material and the CSS — Styling the Web material, specifically: Getting started with HTML What’s in the head? Metadata in HTML HTML text fundamentals Creating hyperlinks Document and website structure Images in HTML What is CSS Getting started with CSS How CSS is structured Of course, you do not need to become an expert in HTML and front-end web development and need to know everything in detail: just as with Vim try to get comfortable enough so that you can adjust existing HTML code to make changes and that if you were to get stuck that you can find a solution using a targeted Google search. Try to play around with the HTML and CSS of your index.html page. You can even use your newly acquired Vim skills to make adjustments to your file. Or you can create an index.html locally and when you are happy copy your local file to the server with scp. When writing HMTL and CSS you will need a text-editor to write your code in. Office document editors are not suitable for this use, as they rely on hidden elements that interfere with the rendering engines used by web browsers. Atom is a good, free, open-source, cross-platform editor. 8.8 Take home message This week we have introduced you to the Shell. At its core, a shell allows you to run programmes from within a textual environment. As a data scientist you will most likely run into this at some point: most likely because you need to run your code or analysis on a dedicated high-performance machine or because you have created some maps that you want to host on a website. This week really only introduced you to the very basics of working with bash and their are many, many more useful command line programmes. Some useful command line programmes that we have used ourselves in the past for a variety of tasks: GNU parallel is a shell tool for executing jobs in parallel using one or more computers. A job can be a single command or a small script that has to be run for each of the lines in the input. The typical input is a list of files, a list of hosts, a list of users, a list of URLs, or a list of tables. A job can also be a command that reads from a pipe. GNU parallel can then split the input and pipe it into commands in parallel. PostgreSQL is a powerful, open source object-relational database. Whilst GUIs are available (e.g. pgAdmin) psql is a terminal-based front-end to PostgreSQL. psql enables you to type in queries interactively, issue them to PostgreSQL, and see the query results. Alternatively, input can be from a file. In addition, it provides a number of meta-commands and various shell-like features to facilitate writing scripts and automating a wide variety of tasks. Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Miniconda is a free minimal installer for conda. It is a small, bootstrap version of Anaconda that includes only conda, Python, the packages they depend on, and a small number of other useful packages, including pip, zlib and a few others.) sed is a stream editor. A stream editor is used to perform basic text transformations on an input stream (a file or input from a pipeline). Mapshaper is software for editing Shapefile, GeoJSON, TopoJSON, csv and several other data formats, written in JavaScript. Although technically written in JavaScript, mapshaper can be installed using npm and run from the command line. ffmpeg is a complete, cross-platform solution to record, convert and stream audio and video. 8.9 Attributions This material was adapted from The Missing Semester of your CS Education [The Shell, Editors (Vim), Data Wrangling). Cambridge, MA: Massachusetts Institute of Technology and is licensed under a Non Commercial-ShareAlike 4.0 International license (CC BY-NC-SA 4.0). [Link] "],
["web-visualisation.html", "9 Web Visualisation", " 9 Web Visualisation This week’s content will be made available on 15/03/2021. "],
["a-look-into-the-future.html", "10 A look into the future", " 10 A look into the future This week we will wrap up what we have discussed over the past weeks and take a look at the future of Social and Geographic Data science. This week’s material will be available on Moodle. "]
]
